{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install spacy\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the text content if number of words is greater than 200 hundred\n",
    "# otherwise the preprocessing will take up a lot of time and memory\n",
    "\n",
    "# Lemmatize, removal punctuations, stopwords and empty strings\n",
    "\n",
    "# Pass the text through the BertTokenizer and then through the TFBertModel and get the features\n",
    "\n",
    "# Pass the feature through the imported prediction model and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For summarization task load the summarizer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"pszemraj/long-t5-tglobal-base-16384-book-summary\")\n",
    "\n",
    "params = {\n",
    "    \"max_length\": 200,\n",
    "    \"min_length\": 50,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    \"repetition_penalty\": 3.5,\n",
    "    \"length_penalty\": 0.3,\n",
    "    \"encoder_no_repeat_ngram_size\": 3,\n",
    "    \"num_beams\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = summarizer(text, **params)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the perprocessing\n",
    "\n",
    "# For lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# For stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For punctuations\n",
    "import re\n",
    "\n",
    "def preprocess(sent):\n",
    "    doc = nlp(sent)\n",
    "    sent = \" \".join([token.lemma_ for token in doc])\n",
    "    sent = re.sub(r\"[^a-zA-Z0-9]\", \" \", sent)\n",
    "    sent = \" \".join([word for word in sent.split(\" \") if word not in stopwords.words('english')])\n",
    "    sent = \" \".join([word for word in sent.split(\" \") if word])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocess(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries for importing the model and getting the prediction\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "# Here the 'bert-base-uncased' model will downloaded once\n",
    "# It gets downloaded automatically when importing the model\n",
    "# The model can also be downloaded separately by cloning the repo\n",
    "# git lfs install\n",
    "# git clone https://huggingface.co/bert-base-uncased\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array(bert_encoder(bert_tokenizer(preprocessed_text, return_tensors='tf', padding=\"max_length\", truncation=True, max_length=200)['input_ids'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the location of the model\n",
    "text_classifier = load_model(\"D:\\\\News Classification Textual\\\\text_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_classifier.predict(encoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
